<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: tdd | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/tdd/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2016-03-28T04:36:51+11:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Refactoring to Improve Testability: Removing Unnecessary Dependencies]]></title>
    <link href="http://rahulpnath.com/blog/refactoring-to-improve-testability-removing-unnecessary-dependencies/"/>
    <updated>2016-03-28T04:27:00+11:00</updated>
    <id>http://rahulpnath.com/blog/refactoring-to-improve-testability-removing-unnecessary-dependencies</id>
    <content type="html"><![CDATA[<p><a href="https://unsplash.com/photos/5Ntkpxqt54Y" class="center" title="Image By Sai Kiran Anagani, from https://unsplash.com/photos/5Ntkpxqt54Y"><img src="/images\refactoring.jpg" class="center" alt="Refactoring"></a></p>

<p>Nowadays I am trying to stick to <a href="http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd">TDD</a> (with the test first approach) and have found it to be of great help. One of the biggest reward doing TDD is that it helps me to <a href="https://vimeo.com/97419151">stay in the flow</a> and regain speed faster after a distraction. This post explains how to refactor code to remove unnecessary dependencies, which is easily found when writing tests.</p>

<p>Unnecessary dependencies are those components which a <a href="http://xunitpatterns.com/SUT.html">SUT</a> depends on, but does not directly affect any of its functionalities. Some of the common tests smell (from XUnit Test Patterns by Gerard Meszaros, <a href="http://www.rahulpnath.com/blog/language-agnostic-books-for-every-developer-2/">a recommended read</a>) that helps me to find these dependencies are <a href="http://xunitpatterns.com/Test%20Code%20Duplication.html">Test Code Duplication</a> and <a href="http://xunitpatterns.com/Fragile%20Test.html">Fragile Tests</a>.</p>

<blockquote><p><em>Cut-and-Paste code reuse for fixture setup happens often when there is an unnecessary dependency.</em></p></blockquote>

<p>While looking for an example to write on, I came across a post from my friend <a href="https://twitter.com/zpbappi">Bappi</a>, where he explains <a href="http://zpbappi.com/testing-codes-with-configurationmanager-appsettings/">Testing Codes with ConfigurationManager</a>. It&rsquo;s a good read on how to remove the dependency with various Configuration Providers by creating an abstraction over it.</p>

<h3>Testability Issues with Current Design</h3>

<p>While abstracting the Configuration Manager by using an interface is a good idea, you should also be careful on how the application classes depend on it. Configurations live at the application root and it is a good idea to restrict dependencies with it at that level. Rest of the application must be dependent only on the configuration value and not the configuration itself. Inner components having dependency with the  configuration provider brings in unnecessary complexities and makes code fragile. Some common issues are</p>

<ul>
<li>Class needs to know of Configuration key</li>
<li>Extra mocking while testing</li>
</ul>


<p>As you see below, the test case from the original post has to set up the Configuration provider mock to return values before testing the class. MyService (assuming that it is not a Factory class, which I confirmed from Bappi) is unnecessarily depending on IAppSettings and coupling itself with the configuration name, which really is not its concern. This leads to brittle code and tests!</p>

<pre><code class="csharp">[Subject(typeof(MyService))]
public class MyServiceTests
{
    Establish context = () =&gt;
        {
            otherDependency = Substitute.For&lt;IMyOtherDependency&gt;();

            var appSettings = Substitute.For&lt;IAppSettings&gt;();
            appSettings["app.name"].Returns("My Test Application");

            myService = new MyService(otherDependency, appSettings);
        };

    Because of = () =&gt; result = myService.PerformOperations();

    It should_call_my_dependency_utility_method_once = () =&gt; otherDependency.Received(1).UtilityMethod();
    It should_execute_successfully = () =&gt; result.ShouldBeTrue();
}
</code></pre>

<h3>Refactoring the Code</h3>

<p>Refactoring such code is as easy as removing the dependency on IAppSettings and taking in the value of &lsquo;app.name&rsquo; as the dependency. This removes the interface dependency and requires only the string value to be passed in. Here I am passing in <a href="https://blogs.msdn.microsoft.com/ploeh/2008/11/17/anonymous-variables/">an anonymous Name</a>, as the value is not of concern for this test.</p>

<pre><code class="csharp">[Subject(typeof(MyService))]
public class MyServiceTests
{
    Establish context = () =&gt;
        {
            otherDependency = Substitute.For&lt;IMyOtherDependency&gt;();
            var anonymousName = "Anonymous Name";
            myService = new MyService(otherDependency, anonymousName);
        };

    Because of = () =&gt; result = myService.PerformOperations();

    It should_call_my_dependency_utility_method_once = () =&gt; otherDependency.Received(1).UtilityMethod();
    It should_execute_successfully = () =&gt; result.ShouldBeTrue();
}
</code></pre>

<blockquote><p><em>When looked at isolation these are minor code changes that hardly removes a line or two. But it has a cumulative effect when applied to all the tests for the class and makes code more robust.</em></p></blockquote>

<p>When looked at isolation, this is a seemingly minor change of not mocking an interface and is just one line of code, which you could live with. But you need to mock that for all tests of that class, which is when you start to see the real benefit. Also, you have made the tests more resilient by not taking an unnecessary dependency. Even if you decide to change the configuration name to &lsquo;<em>ApplicationName</em>&rsquo;, none of the tests break now, whereas with the original code all of them would have.</p>

<p><em>One possible argument with this refactoring is, <strong> What if I need an extra value from the dependency (app.domain in the above case), I now have to update the class constructor</strong>.</em></p>

<p>Agreed, but then this violates <a href="https://blog.8thlight.com/uncle-bob/2014/05/12/TheOpenClosedPrinciple.html">Open Closed Principle</a>, which states &lsquo;You should be able to extend a classes behaviour without modifying it.&rsquo; If you need a new configuration value, you are essentially changing the components functionality, so you should either extend current functionality or write a new component. This also opens up a hidden code smell with the existing code and an anti-pattern - <a href="http://blog.ploeh.dk/2010/02/03/ServiceLocatorisanAnti-Pattern/">Service Locator</a>. So the refactoring still holds good!</p>

<p>Hope this helps you find dependencies with unnecessary components and remove them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Is Code Coverage a Lie?]]></title>
    <link href="http://rahulpnath.com/blog/is-code-coverage-a-lie/"/>
    <updated>2015-04-11T23:30:42+10:00</updated>
    <id>http://rahulpnath.com/blog/is-code-coverage-a-lie</id>
    <content type="html"><![CDATA[<p>Code Coverage has been one of the important things that was always there on a daily build report and a number that managers were both interested and worried about. This has been the same for all projects that I had worked on, across various organizations. Various tools support measuring coverage and can be included as part of the build reports, which essentially indicates the percentage of source code that is covered by &lsquo;unit tests&rsquo;. A higher percentage obviously indicates a better tested code and more stable one is what one would expect, as that would be the sole purpose of measuring this. But how far is this true? Is a code with higher test percentage one with lesser bugs? Does 100% indicate that it is totally bug free or is this a total wrong way of seeing that number?</p>

<h3>What is Code Coverage and How is it Measured?</h3>

<p>Code Coverage indicates the percentage of code that has been covered by at least one or more tests in your test suite. So if you have a simple function like the one shown below, which just divides two numbers and if you have one test that invokes that function, then you have a code coverage of 100%. When using Visual studio tests (as that is what I have used to depict the below one), code coverage is measured either based on blocks of code or based on lines of code.</p>

<ul>
<li><h4>Block-based statement coverage</h4>

<p>For the purposes of the tools, a block is defined as a sequence of instructions that have a single entry point and a single exit point. Exit points include branch instructions, a function call, a return instruction, or, for managed code, a throw instruction.</p></li>
<li><h4>Line-based coverage</h4>

<p>For line-based coverage, the tools identify all of the blocks that make up a line and then use this information to determine the level of coverage for the line. If all of the blocks that make up the line are covered, then the tools report that the line is covered. If no blocks in the line are covered, then the tools report that the line is not covered. If some, but not all, of the blocks in the line are covered, then the tools report that the line is partially covered.</p></li>
</ul>


<p>In the below case we have both 100% as there is only one line and one block and the test covers both. So should managers be happy with this code, since it has 100% coverage? Maybe they are, but ask the developers, they definitely are not, or probably they are if at the first place they wrote the tests for the sake of the report and since the managers are happy they too are. But is this what we should strive for - definitely not!</p>

<p><img class="center" alt="Visual Studio Code Coverage" src="/images/code_coverage.PNG" /></p>

<h3>Trustworthy Unit Tests</h3>

<blockquote><p>Unit tests should be like a parachute - trustworthy. You are not going to jump out from an airplane with one that you know is faulty.</p></blockquote>

<p>Having unit tests that are run once in a while or just for the build reports or when the manager is at your back, are a complete waste of time. A set of faulty tests do more harm than having none, as it might give wrong judgment of the code. From the above code you can definitely find that though the coverage is 100%, the code would simply fail when a value of 0 is passed for &lsquo;b&rsquo;, and it is highly likely that this would happen too.</p>

<blockquote><p>If unit tests are written just to meet the code coverage number on a report, then I would rather fake the report than writing the tests</p></blockquote>

<p>Unit Tests really make sense only when you write them following &lsquo;<a href="http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd">The Three Rules of TDD</a>&rsquo; or at the very least you have tests associated with every change that you make and also you make sure that we run the entire test suite before a check-in to make sure that you have broken nothing with your changes. Only when there is such &lsquo;trustworthiness&rsquo; in the tests, does it make sense to have them in the first place and not for any number on a report.</p>

<h3>How should we really see that number on the report?</h3>

<p>Now that we have seen it is all about having trustworthy tests and not about reaching a targeted number for a report, what should we really use this Code coverage report data for?</p>

<ol>
<li>If you are less than 100% it clearly indicates that there are areas of your application that is not tested. You would really want to cross check if that is an intended miss or if there is something that is actually missing and add in some more tests to be covered better.</li>
<li>When refactoring code, you can always make sure that you don&rsquo;t go back on the coverage number that you started with, so as to ensure that you have not introduced untested code into the system.</li>
<li>When cleaning up tests a drop in the coverage number clearly indicates that you have actually removed some valid and non-redundant tests.</li>
</ol>


<p>So the next time you add in a test, think why you are doing it. Is it for the report or for the application that is getting developed. Lets all <a href="https://vimeo.com/43536488">strive to be really professional</a> in what we are doing and not just hide behind some numbers. Let the number be seen for what it is - nothing less and nothing more!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TDD and Refactoring]]></title>
    <link href="http://rahulpnath.com/blog/tdd-and-refactoring/"/>
    <updated>2010-08-23T20:42:00+10:00</updated>
    <id>http://rahulpnath.com/blog/tdd-and-refactoring</id>
    <content type="html"><![CDATA[<p>Over the days I have been reading on Test Driven Development(TDD) and it seems really interesting methodology to go with as per development is concerned.<br/>
Basic of TDD is that the development process relies on &lsquo;tests&rsquo;, that are written prior to code.<br/>
Sounds astonishing!!!!<br/>
It might to someone into the normal development mode,where tests are usually written after code, so as to match the code that is written.<br/>
But TDD says just the opposite.<br/>
Code to make the tests pass&hellip;Just pass..<em>Nothing more and Nothing less</em>.<br/>
Thats where the catch is where most of we developers might find it difficult and needs getting used to.Not getting more into it as I would not be the best to comment on it :)<br/>
So whats refactoring got to do here.<br/>
Refactoring plays an integral part of TDD,so that the code is elegant and conveys just what it needs to,avoiding duplication.<br/>
The TDD approach also assists in refactoring as you have tests readily available to assure that the functional behaviour is not affected while changing the code design.You are just click of a button away if you are having a automated test scripts (like nUnit) in veryfying refactoring. <br/>
So both TDD and Refactoring goes hand in hand and helps in greatly improving the overall code quality.<br/>
There are quite a lot resources out there on these.<br/>
The best for refactoring would be of <a href="http://www.amazon.com/gp/product/0201485672/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0201485672&amp;linkCode=as2&amp;tag=rahulpnath-20">Refactoring: Improving the Design of Existing Code</a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=rahulpnath-20&amp;l=as2&amp;o=1&amp;a=0201485672" alt="" />.<br/>
For TDD <a href="http://www.amazon.com/gp/product/0735619484/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0735619484&amp;linkCode=as2&amp;tag=rahulpnath-20">Test-Driven Development in Microsoft  .NET (Microsoft Professional)</a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=rahulpnath-20&amp;l=as2&amp;o=1&amp;a=0735619484" alt="" /> would be a good start and also the one by Kent Beck <a href="http://www.amazon.com/gp/product/0321146530/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0321146530&amp;linkCode=as2&amp;tag=rahulpnath-20">Test Driven Development: By Example</a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=rahulpnath-20&amp;l=as2&amp;o=1&amp;a=0321146530" alt="" /></p>
]]></content>
  </entry>
  
</feed>
