<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: testing | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/testing/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2016-03-11T14:27:52+11:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Fiddler to help in Manual Testing]]></title>
    <link href="http://rahulpnath.com/blog/using-fiddler-to-help-in-manual-testing/"/>
    <updated>2016-03-07T17:33:00+11:00</updated>
    <id>http://rahulpnath.com/blog/using-fiddler-to-help-in-manual-testing</id>
    <content type="html"><![CDATA[<p>Fiddler is an HTTP debugging proxy server application, that captures HTTP and HTTPS traffic and displays to the user. It also enables modifying HTTP traffic when sent or received. Fiddler is <a href="http://www.rahulpnath.com/blog/tools-that-I-use/">one of the tools that I use daily</a> and is an indispensable one for any web developer.</p>

<p>This post gives an introduction on how you can use fiddler to help with &lsquo;manual testing&rsquo;. We will see how to use Fiddler to create requests to Web API,  modify and replay an existing request. We will also see how to test error scenarios to see how the application functions in those cases. The sample solution is the default Web API project in Visual Studio with a few changes.</p>

<h3>Composing a Request</h3>

<p>When testing API&rsquo;s to see how it behaves with various inputs, one often needs to send in different parameters. Fiddler allows composing new requests and  modifying existing ones.</p>

<p>Using the Fiddler composer window (shown in the image below), we can create new requests from scratch and execute them. It provides two modes to create requests:</p>

<ul>
<li>Parsed : This is an assisted form to create requests</li>
<li>Raw : This allows to create raw http requests and issue them.</li>
</ul>


<p>Fiddler also allows saving raw requests in the Scratchpad tab to execute as and when required. On clicking Execute Fiddler creates an HTTP request from the entered data and sends to the server. To modify requests you can either drag and drop the request from the displayed URL&rsquo;s list into the composer tab or right-click on an entry and <em>Unlock for Editing</em> (keyboard shortcut - F2). After making the changes to the request in the Inspector window, right-click on the request again to Replay -> Reissue ( R).</p>

<p><img class="center" alt="Fiddler Composer tab" src="/images/fiddler_composer.png" /></p>

<h3>Testing Error Cases</h3>

<p>Testing error cases is tricky, especially from a UI level. Things usually don&rsquo;t go wrong in the development/testing environment and <a href="http://blog.codinghorror.com/the-works-on-my-machine-certification-program/">almost never on a developers machine</a> which makes it very hard to test for cases where something does not work. Fiddler makes it easy to test error scenarios with <a href="http://docs.telerik.com/fiddler/KnowledgeBase/AutoResponder">AutoResponder</a>, which allows returning handcrafted responses for requests, without actually hitting the server.</p>

<p>To create an auto response for a URL, select the URL from the URL&rsquo;s list and drag it into the AutoResponder tab or select the URL and click on Add Rule button on AutoResponder tab, which will create a new rule. By default Fiddler creates a rule with an exact match (Exact:) with the selected URL. Fiddler supports different <a href="http://docs.telerik.com/fiddler/KnowledgeBase/AutoResponder#matching-rules">matching rules</a> which include regular expression matches. A list of default response text are available to choose from to respond to requests that match the URL matching rule. We can also create a custom response and save it for reuse. The next time a request with matching URL is found the custom response gets returned to the caller.</p>

<blockquote><p><em>Make sure that the &lsquo;Unmatched requests passthrough&rsquo; option is true in the AutoResponder tab to make sure that all other requests pass through to the server.</em></p></blockquote>

<p><img class="center" alt="Fiddler AutoResponder tab" src="/images/fiddler_autoresponder.png" /></p>

<p>To create a custom response, choose &lsquo;Create a New Response&rsquo; or &lsquo;Find a file&rsquo; (if you already have the response saved in a text file). You can save custom responses in the <em>ResponseTemplates</em> folder in the root folder of Fiddler installation, to have them populated in the AutoResponder tab. When editing existing response data, make sure properties like Content-Length reflects the correct values. You can also set a <a href="http://docs.telerik.com/fiddler/KnowledgeBase/AutoResponder#latency">Latency</a> for the response, to simulate response coming from a server. RIght click on the rules for the Set Latency option and enter the value in milliseconds.</p>

<p>With the AutoResponder set to matching URL, we can easily have it return error codes or simulated error messages to test how the UI handles them. You don&rsquo;t have to depend on &lsquo;actual server errors&rsquo; to test if the UI handles error correctly. You can use this to test how application behaves with different return values by mocking with valid custom responses.  Fiddler provides richer capabilities of using scripts to <a href="http://docs.telerik.com/fiddler/KnowledgeBase/FiddlerScript/ModifyRequestOrResponse">modify a request or response</a>.</p>

<p>Hope this helps you get started with using Fiddler for testing and manipulating requests/responses.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Organizing Tests into Test Suites for Visual Studio]]></title>
    <link href="http://rahulpnath.com/blog/organizing-tests-into-test-suites-for-visual-studio/"/>
    <updated>2016-01-18T22:51:03+11:00</updated>
    <id>http://rahulpnath.com/blog/organizing-tests-into-test-suites-for-visual-studio</id>
    <content type="html"><![CDATA[<p>While working with large code base, that has a lot of tests (unit, integration, acceptance etc), running all of them every time we make a small  change (if you are doing TDD or just using build for feedback) takes a lot of time. Organizing tests into different test suites, making it easier to run as required by the current context, is handy in such cases.</p>

<p>There are multiple ways that we can do this within Visual Studio and below are some of the options available. I tend to use a mix of all these in my current project. This gives the flexibility to run only the new tests that I am writing while writing new code or set of related tests for the updates that I am making. Once done with the changes, I can run the full suite of unit tests, followed by the integration tests. This reduces the interruption duration while coding and has a direct impact on the overall productivity too. (If you think small interruptions does not matter much think twice!)</p>

<p><img class="center" alt="Geek productivity" src="/images/geek_productivity.jpg" /></p>

<h4><strong>Test Traits</strong></h4>

<p>Traits are a good way to group tests together and to run them as different suites. It encompasses TestCategory, TestProperty, Priority and Owner. Using <a href="https://msdn.microsoft.com/en-au/library/microsoft.visualstudio.testtools.unittesting.testcategoryattribute.aspx">TestCategory</a> attribute we can specify  the group of the test and the Visual Studio Test Explorer uses this value to group the tests and allows executing tests in specific groups.</p>

<p><img class="center" alt="Visual Studio Test Traits" width="75%" src="/images/vs_testExplorer_traits.png" /></p>

<p>Limitation with the above approach is that it depends on developers to put these attributes on the test cases or class level and not leveraging any existing conventions that might be already in place. Having integration tests, unit tests, acceptance tests in different projects is a very common practice, with conventions like project names ending with &lsquo;.UnitTests, .IntegrationTests, .AcceptanceTests&rsquo; etc.</p>

<h4><strong>Build Tasks and Task Runner Explorer</strong></h4>

<p>The <a href="https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708">Task Runner Explorer</a> (TRE) provides custom task runner support to Visual Studio, allowing to run grunt/gulp task or target inside Visual Studio. Grunt/Gulp has packages for most of the unit testing frameworks, using which different build tasks can be created. To select the tests to execute different conventions can also be used. Below is an example of a gulp task to execute all the c# unit tests in the project.</p>

<pre><code class="javascript">var gulp = require('gulp');
var xunit = require('gulp-xunit-runner');
var xunitConsolePath = 'xunit.console.exe';
var unitTestsConvention = ['**/*.Tests.dll'];

gulp.task('c#UnitTests', function () {
    runTests(unitTestsConvention);
});

function runTests(dllPath) {
    return gulp.src(dllPath, { read: false })
        .pipe(xunit({
            executable: xunitConsolePath,
            options: { parallel: 'all' }
        }));
}
</code></pre>

<p>Similarly we can have multiple tasks to execute different groups of tests and it will be available in the TRE within Visual Studio as shown below. This approach gives the most flexibility, allowing tests be grouped any way and providing ability to execute tests across the stack of technologies.
<img class="center" alt="Visual Studio Task Runner Explorer" src="/images/vs_tre.png" /></p>

<h4><strong>Tests Settings File</strong></h4>

<p>Creating Test Playlist is an easy way to group tests into a playlist and executing them as  group. From the Test Explorer, select the tests to be grouped and on right-click, the option to create playlist is available. The saved playlists can be selected from the drop down menu on the top bar for later execution.</p>

<p><img class="center" alt="Visual Studio Test Playlist" src="/images/vs_testExplorer_playlist.png" /></p>

<p>This works well for short-lived groupings, when we are actively working on a part of the code and need to execute tests for that area. Every time a new test is added, we need to add it explicitly to the playlist if required.</p>

<p>We have seen multiple ways of grouping tests into test suites, and each of them comes handy in different situations. For project wide convention tests, I tend to use build tasks that integrate with TRE as it is more flexible and extendable. Do you use any other ways to group your tests, drop in with a comment!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Multiple Implementations of same Interface]]></title>
    <link href="http://rahulpnath.com/blog/testing-multiple-implementations-of-same-interface/"/>
    <updated>2015-01-10T15:54:15+11:00</updated>
    <id>http://rahulpnath.com/blog/testing-multiple-implementations-of-same-interface</id>
    <content type="html"><![CDATA[<p>Often there are times when we need to test multiple implementations of the same interface. We would want to use the same test case against all the implementations so that we <a href="http://en.wikipedia.org/wiki/Don%27t_repeat_yourself">don&rsquo;t repeat ourselves</a>. In this post we will see how we can reuse the same test cases to test both the implementation, by running them against both the implementations.</p>

<blockquote><p>If you are just interested in the approach - The same test project dll is run twice using vstest.console, by setting an environment variable. Inside the test, (either in the assembly initialize or test initialize) register the appropriate implementations into a IoC container, based on the environment variable value.</p></blockquote>

<p>Interested in the full implementation, then read on!</p>

<p>Since we are not much bothered about the actual interface and its implementation, I have a very simple interface as below, which calculates the length of the given string.There are two implementations for this that might have two different ways of calculating the length of the string given an input.</p>

<pre><code class="csharp">public interface IFoo
{
    int GetLength(string input);
}
</code></pre>

<pre><code class="csharp Implementation 1">public class Foo : IFoo
{
    public int GetLength(string input)
    {
        return input.Count();
    }
}
</code></pre>

<pre><code class="csharp Implementation 2">public class Foo : IFoo
{
    public int GetLength(string input)
    {
        return input.Length;
    }
}
</code></pre>

<p>Though the sample has a simple interface, this might not be the case in a real life project. So the sample mimics a real time implementation structure - we have one interface project and two other projects that have the corresponding implementation. The implementations could also be in the same assembly and this would be applicable for those scenarios too, and can be made to work with some few tweaks in one of the steps (which I will mention when we are there). The test case project that will have the appropriate test cases.</p>

<pre><code class="csharp">[TestMethod]
public void TestThreeLetterLength()
{
    var foo = this.container.Resolve&lt;IFoo&gt;();
    var returnValue = foo.GetLength("Foo");
    Assert.IsTrue(returnValue == 3);
}
</code></pre>

<p>The test case uses the IoC container to get the corresponding implementation of the interface, so it is not all about switching the registered implementation in the container. If this is only for the tests in this particular class then we could do this in the <a href="http://msdn.microsoft.com/en-us/library/microsoft.visualstudio.testtools.unittesting.testinitializeattribute.aspx">TestInitialize</a> method. But most likely you would have multiple tests and also multiple interfaces that we are using. So we can do this in the <a href="http://msdn.microsoft.com/en-us/library/microsoft.visualstudio.testtools.unittesting.assemblyinitializeattribute.aspx">AssemblyInitialze</a> for the assembly.</p>

<pre><code class="csharp Interface">var test = Environment.GetEnvironmentVariable(TestEnviromentVariable);

if (test == "1")
{
    container.RegisterType&lt;IFoo, FooImplementation1.Foo&gt;();
}
else if (test == "2")
{
    container.RegisterType&lt;IFoo, FooImplementation2.Foo&gt;();
}
</code></pre>

<p>The above implementation might work in cases where the number of interfaces are less and also in cases where we have fewer possibilities of implementations, but as soon as the number goes up we will again have to keep repeating  the registrations and the if/else code. This is an IoC registration issue and is best handled using <a href="http://www.rahulpnath.com/blog/ioc-registration-by-convention/">IoC Registration by Convention</a>. We can have a configuration file matching the environment variable and have the assemblies that are to be loaded mentioned in that and pass only those assemblies to be explicitly registered into the convention registration logic. Even in cases where you have the implementations in the same assembly you can write your convention registration logics accordingly and decide what to register.</p>

<p>We can now run these test dll&rsquo;s using batch files by setting different environment variables as below. The bat files can be integrated into your build</p>

<pre><code class="bat FooTest.Implementation2.bat">set Foo.tests=2
echo "Testing for configuration 2"
msbuild TestingMultipleImplementations.sln
vstest.console FooTestImpl1\bin\Debug\FooTestImpl1.dll /logger:trx
</code></pre>

<p>Hope this helps some one trying to reuse test cases for multiple implementations of the same interface. One another way to solve this issue would be to create multiple csproj files and have the same test case classes referred to both the project files, but have the reference assemblies specific to implementations. So in this case we would have multiple test dll&rsquo;s created, which can be run individually. The advantage of going via this approach is that we could have test cases specific to implementations too and also reuse test cases that are same across implementations by referring them as linked files. But currently we did not want this flexibility and did not want to add multiple project files and make it difficult for the team. You can find the sample implementation <a href="https://github.com/rahulpnath/Blog/tree/master/TestingMultipleImplementations">here</a>. Do you reuse test cases like this? Do drop in with a comment on your thoughts.</p>
]]></content>
  </entry>
  
</feed>
