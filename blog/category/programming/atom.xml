<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2016-07-11T05:48:31+10:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Protect Yourself Against Line Ending Issues when Using Environment.Newline to Split Text]]></title>
    <link href="http://rahulpnath.com/blog/protect-yourself-against-line-ending-issues-when-using-environment-dot-newline-to-split-text/"/>
    <updated>2016-07-11T05:45:31+10:00</updated>
    <id>http://rahulpnath.com/blog/protect-yourself-against-line-ending-issues-when-using-environment-dot-newline-to-split-text</id>
    <content type="html"><![CDATA[<blockquote><p><em>In computing, a <a href="https://en.wikipedia.org/wiki/Newline">newline</a>, also known as a line ending, end of line (EOL), or line break, is a special character or sequence of characters signifying the end of a line of text and the start of a new line. The actual codes representing a newline vary across operating systems, which can be a problem when exchanging text files between systems with different newline representations.</em></p></blockquote>

<p>I was using a Resource (resx) file to store large text of comma separated values (CSV). This key-value mapping represented the mapping of product codes between an old and new system. In code, I split this whole text using <a href="https://msdn.microsoft.com/en-us/library/system.environment.newline(v=vs.110).aspx">Environment.NewLine</a> and then by comma to generate the map, as shown below.</p>

<pre><code class="csharp">AllMappings = Resources.UsageMap
    .Split(new string[] { Environment.NewLine }, StringSplitOptions.RemoveEmptyEntries)
    .Select(s =&gt; s.Split(new[] { ',' }))
    .ToDictionary(item =&gt; item[0], item =&gt; item[1]);
</code></pre>

<p>It all worked fine on my machine and even on other team members machines. There was no reason to doubt this piece of code, until on the development environment we noticed the mapped value in the destination system always null.</p>

<h3>Analyzing the Issue</h3>

<p>Since in the destination system, all the other values were getting populated as expected, except for this mapping it was easy to narrow down to the class that returned the mapping value, to be the problematic one. Initially, I thought this was an issue with the resource file not getting bundled properly. I used <a href="https://www.jetbrains.com/decompiler/">dotPeek</a> to decompile the application and verified that resource file was getting bundled properly and had exactly the same text (visually) as expected.</p>

<p><img src="/images/newline_dotpeek.png" alt ="Resource file disassembled in dotPeek" /></p>

<p>I copied the resource file text from disassembled code in dotPeek into <a href="http://www.flos-freeware.ch/notepad2.html">Notepad2</a> (configured to show the line endings) and everything started falling into place. The resource text file from the build generated code ended with LF (\n), while the one on our development machines had CRLF (\r\n). All machines, including the build machines are running Windows and the expected value for <a href="https://msdn.microsoft.com/en-us/library/system.environment.newline(v=vs.110).aspx">Environemnt.Newline</a> is CRLF - <strong> A string containing &ldquo;\r\n&rdquo; for non-Unix platforms, or a string containing &ldquo;\n&rdquo; for Unix platforms.</strong></p>

<p><figure>
<img src="/images/newline_diff.png" alt ="Difference between build generated and development machine resource file" />
<figcaption><em>Difference between build generated and development machine resource file</em></figcaption>
</figure></p>

<h3>Finding the Root Cause</h3>

<p>We use git for our source control and <a href="https://help.github.com/articles/dealing-with-line-endings/">configured to use &lsquo;auto&rsquo; line endings</a> at the repository level. This ensures that the source code, when checked out, matches the line ending format of the machine. We use <a href="https://www.atlassian.com/software/bamboo">Bamboo</a> on our build servers running Windows. The checked out files on the build server had LF line endings, which in turn gets compiled into the assembly.</p>

<p>The checkout step in Bamboo used the built in git plugin (JGit) and has certain limitations. It&rsquo;s recommended to use native git to use the full git features. JGit also has a known issue with <a href="https://jira.atlassian.com/plugins/servlet/mobile#issue/BAM-9591">line endings on a Windows machine</a> and checks out a file with LF endings. So whenever the source code was checked out, it replaced all line endings in the file with LF before compilation. So the resource file ended up having LF line endings in the assembly, and the code could no longer find Environment.Newline (\r\n) to split.</p>

<h3>Possible Fixes</h3>

<p>Two possible ways to fix this issue is</p>

<ul>
<li>Switch to using native git on the bamboo build process</li>
<li>Use LF to split the text and trim any excess characters. This reduces dependency on line endings variations and settings between different machines only until we are on a different machine which has a different format.</li>
</ul>


<p>I chose to use LF to split the text and trim any additional characters, while also <a href="https://confluence.atlassian.com/bamboo/defining-a-new-executable-capability-289277164.html">updating Bamboo to use native git</a> for checkout.</p>

<pre><code class="csharp">AllMappings = Resources.UsageMap
    .Split(new string[] {"\n"}, StringSplitOptions.RemoveEmptyEntries)
    .Select(s =&gt; s.Split(new[] { ',' }))
    .ToDictionary(item =&gt; item[0].Trim().ToUpper(), item =&gt; item[1].Trim());
</code></pre>

<h3>Protecting Against Line Endings</h3>

<p>The easiest and fastest way that this would have come to my notice was to have a unit test in place. This would ensure that the test fails on the build machine. A test like below will pass on my local but not on the build machine as UsageMap would not return any value for the destination system.</p>

<pre><code class="csharp">[Theory]
[InlineData("MovieWeek", "Weekly-Movie")]
[InlineData("Dell15", "Laptop-Group3")]
public void SutReturnsExpected(string sourceSystemCode, string expected)
{
    var sut = new UsageMap();
    var actual = sut.GetDestinationCode(sourceSystemCode);
    Assert.Equal(expected, actual);
}
</code></pre>

<p>Since there are different systems with different line endings and also applications with different line ending settings and issues of its own, there does not seem to be a &lsquo;one fix for all&rsquo; cases. The best I can think of in these cases is it protect us with such unit tests. It fails fast and brings it immediately to out notice. Have you ever had to deal with an issue with line endings and found better ways to handle them?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Make it Easy for the New Person Joining the Team - Have a Project Ramp up Plan]]></title>
    <link href="http://rahulpnath.com/blog/make-it-easy-for-the-new-person-joining-your-team-have-a-project-ramp-up-plan/"/>
    <updated>2016-05-18T12:38:24+10:00</updated>
    <id>http://rahulpnath.com/blog/make-it-easy-for-the-new-person-joining-your-team-have-a-project-ramp-up-plan</id>
    <content type="html"><![CDATA[<p>Recently I was in a discussion with my friend/colleague on conducting a few ramp up sessions for the new hires in our team. The discussion went as below,</p>

<blockquote><p><em>Me: We should hold a few sessions to make the new guys in team more comfortable</em></p>

<p><em>Friend: It&rsquo;s too early for it. We should let them find their own way and not &lsquo;spoon-feed&rsquo; them with information.</em></p>

<p><em>Me: But we are not &lsquo;spoon-feeding&rsquo; them, we are just making their learning process faster and giving then an overview on how all the technology fits together in our world of things.</em></p>

<p><em>Friend: But &lsquo;I did not have any ramp up when I joined, and I felt it was better to have learned it on my own, though it took a lot more time.</em></p></blockquote>

<p><a href="http://www.mindtickle.com/wp-content/uploads/2014/02/new_employee_orientation_business_strategy_research.png" class="center" title="Image, from http://www.mindtickle.com/wp-content/uploads/2014/02/new_employee_orientation_business_strategy_research.png"><img src="/images\rampup_plan.png" class="center" alt="Rampup Plan"></a></p>

<p>Just like there are company-wide induction/onboarding sessions, I have always felt that project specific onboarding plans are also required and help new hires be part of the team and be more productive with their day-to-day activities faster. As mentioned in this <a href="http://www.fastcompany.com/3029820/work-smart/infographic-the-real-ways-to-hold-on-to-new-hires/3">article</a>, <em>New hires care more about effective job training and clear guidelines, and it&rsquo;s time you provide that for them.</em> It&rsquo;s best to have a plan in place when you have someone new joining your team and you along with the team are the best people to put that plan together.</p>

<h3>Boy Scout Rule</h3>

<p>The Boy Scouts have a <a href="http://programmer.97things.oreilly.com/wiki/index.php/The_Boy_Scout_Rule">rule</a> - <em>&ldquo;Always leave the campground cleaner than you found it.&rdquo;</em>  New hires are like &lsquo;new camp group&rsquo; at a campground, so it&rsquo;s the duty of the &lsquo;existing team&rsquo; there to make it a good experience for them.</p>

<blockquote><p><em>&lsquo;Refactor&rsquo; your experiences to make it better for the next person who is about to take on the same journey</em></p></blockquote>

<p>It&rsquo;s not that my friend was intentionally trying not to pass on any information, but he felt that learning on their own would be better. Even I agree with him that learning on your own is far better than &lsquo;spoon-feeding&rsquo; - but a ramp up plan is not spoon-feeding. A ramp up plan is only to speed up the learning process and to make it more comfortable for someone joining new.</p>

<h3>When to create the plan?</h3>

<p>The need for such a plan is there only when there has been enough progress made on the project, after which there is someone new joining the team. So when a new hire is scheduled to join is a good time to create the &lsquo;draft&rsquo; plan. Once the new hire has gone through it and updated back with his own experiences it could become the first version of the plan, which can then be confidently shared to anyone joining after as it has worked for at least one person.</p>

<h3>What should be there in the plan</h3>

<p><em>It depends!</em></p>

<p>It&rsquo;s totally up to the team to decide what should be there in the plan. Some of the things that I usually have are</p>

<ul>
<li><p><strong>Overview of the project and what problem it is trying to solve</strong></p>

<p>It&rsquo;s really important that everyone on the team knows what the application is trying to solve and have a common goal to work towards. It&rsquo;s not just about the code we write but about the problem we are solving and that needs to be clearly defined. I would record a video, when this is done the first time and share it with anyone joining after that, as most of the core concepts of a project rarely change. There could be a follow-up session post watching the video, to also have a quick walk through and fill any missing gaps.</p></li>
<li><p><strong>Introduction to various technologies used in the project and how everything fits together</strong></p>

<p>Technology changes so fast these days that it is nearly impossible to stay updated with all the available options. So a walk through of the different technologies and pointers to resources that worked for you and the team will be of help. If there are any specific libraries, frameworks getting used, an introduction to those should also help.</p></li>
<li><p><strong>Release cycle and Release management</strong>
Every project has its own model of delivering the end product and everyone on the team should understand this process well. Having a continuous build is becoming more common these days and helps reduce the complexity of release. An end to end walk-through of the deployment process helps understand the application better and provides exposure to all the moving parts in the system.</p></li>
<li><p><strong>Environment/Machine setup</strong>
Software installation is one of the biggest pain when setting up a new machine for a project, especially with having specific versions of the software. Having a documented list of all the project dependencies (hardware and software) makes setting the project environment easy. It&rsquo;s preferable to have these <a href="https://chocolatey.org/">scripted</a>. Have a common place, where you can find links to all the various environments (dev, at,prod etc) and related resources.</p></li>
<li><p><strong>Patterns and Conventions</strong>
Every project has its own conventions and certain core patterns that are followed. It&rsquo;s good to have these patterns available for reference so that it helps understand the code better and helps reduce code-review cycles. Than having one big boring document, what I prefer more is to have multiple blog articles targeting each of those. I try to generalize commonly used patterns in the projects that I have worked on and create blog posts. This also helps generate content for <a href="http://www.rahulpnath.com/blog/get-started-with-your-blog/">your blog</a>.</p></li>
<li><p><strong>Tips &amp; Tricks</strong>
This could range from how to easily navigate the code base, scripts to do some commonly occurring task and general things to keep an eye for.</p></li>
</ul>


<p>These are just some of the things I generally try to include in a ramp up plan but as said it totally depends on the team and the project.</p>

<h3>Sharing the plan</h3>

<p>Depending on the plan, if it has confidential information, you could split this into two (or more) different documents and share it at different phases of onboarding. Once a new hire is confirmed it&rsquo;s good to share the parts which do not have any confidential information. Technology stack, conventions used, machine setup (<a href="https://en.wikipedia.org/wiki/Bring_your_own_device">BYOD</a>) are usually not confidential and can be shared well before actual employment. Once all employment agreements are in place the rest too can be shared. It&rsquo;s also a good idea to have some walk-through of the plan itself to make it easier to follow.</p>

<h3>Iterate and Improve</h3>

<p>Updating back with the experiences of the people using the plan is important to keep it current and valuable. Suggesting improvements and updates should be an item in the plan so that this does not get missed. To make updates manageable, the plan must be accessible to all and preferably version controlled if they are documents.</p>

<p>What are your thoughts on having a ramp up plan?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Disable NuGet Package Restore for a .Net Poject]]></title>
    <link href="http://rahulpnath.com/blog/disable-nuget-package-restore-for-a-net-poject/"/>
    <updated>2016-05-02T13:26:59+10:00</updated>
    <id>http://rahulpnath.com/blog/disable-nuget-package-restore-for-a-net-poject</id>
    <content type="html"><![CDATA[<p><em>If you have decided on <a href="http://www.rahulpnath.com/blog/checking-in-package-dependencies-into-source-control/">Checking in Package Dependencies into Source Control</a> for an existing project that uses Nuget Packages then this post is for you</em></p>

<p>When using NuGet package references that are not included in the source control, these packages gets restored during build time. There are <a href="https://docs.nuget.org/consume/package-restore">multiple ways that NuGet supports restore these dependencies at build time</a></p>

<ul>
<li>Automatic Package Restore is the current recommended approach (within Visual Studio), which is available from NuGet 2.7.</li>
<li>Command-line package restore on build servers</li>
<li>MSBuild-integrated package restore approach is the original Package Restore implementation and is still used in many projects.</li>
</ul>


<p>Depending on the type to of restore the project uses, NuGet has different configuration entries in the <em>csproj</em> files and <em>.nuget</em> folder in the solution root. So when choosing to check in package dependencies into the source control, it is a good idea to remove all these <a href="https://docs.nuget.org/consume/package-restore/migrating-to-automatic-package-restore">generated configurations</a> and files that are not required any more. The below script does this for you!</p>

<div class="alert alert-warning">
<strong>WARNING!</strong> The script deletes the <em>.nuget</em> folder (if it exists), updates the <em>.csproj</em> files. Please make sure that the project folder is under source control or you have a backup of the folder. After running the script make sure that all the changes that you see are expected as explained here and the project builds and runs as before.
</div>


<p>The PowerShell script does the below for a given solution directory folder (mandatory)</p>

<ul>
<li>For each of the <em>csproj</em> file in the given folder, the script removes the

<ul>
<li><em>RestorePackages</em> node</li>
<li><em>NugetPackageImportStamp</em> node</li>
<li><em>nuget target import</em> from the solution root .nuget folder</li>
<li><em>EnsureNuGetPackageBuildImports</em> node</li>
</ul>
</li>
<li>Removes <em>.nuget</em> folder from the solution root if it exists.</li>
</ul>


<p><em>The script leaves blank lines in the </em>csproj<em> files in place of the removed nodes.</em></p>

<pre><code class="powershell Remove NuGet Restore https://gist.github.com/rahulpnath/13d3b4f54cec51e22344876b1566b911#file-remove-nuget-restore-ps1">param([Parameter(Mandatory=$true)][string]$solutionDirectory) 

 $importNugetTargetsTag= [regex]::escape(@'
&lt;Import Project="$(SolutionDir)\.nuget\NuGet.targets" Condition="Exists('$(SolutionDir)\.nuget\NuGet.targets')" /&gt;
'@)

$restorePackagesTag = '&lt;RestorePackages&gt;.*?&lt;/RestorePackages&gt;'
$nuGetPackageImportStamp = '&lt;NuGetPackageImportStamp&gt;.*?&lt;/NuGetPackageImportStamp&gt;'

$EnsureNuGetPackageBuildImportsTargetTag = '(?smi)&lt;Target Name="EnsureNuGetPackageBuildImports".*?&lt;/Target&gt;'

foreach ($f in Get-ChildItem -Recurse -Path $solutionDirectory -Filter *.csproj | sort-object)
{
    $text = Get-Content $f.FullName -Raw
    $text `
        -replace $importNugetTargetsTag, "" `
        -replace $nuGetPackageImportStamp, "" `
        -replace $restorePackagesTag, "" `
        -replace $EnsureNuGetPackageBuildImportsTargetTag, "" `
        | set-content $f.FullName
}

Get-ChildItem -Path $solutionDirectory -include .nuget -Recurse | foreach ($_) { remove-item $_.fullname -Force -Recurse }
</code></pre>

<p>Any similarity with the scripts <a href="http://weblogs.asp.net/jongalloway/scripting-net-project-migration-to-automatic-nuget-package-restore">here</a> is intended as that was my starting place. To explicitly <a href="https://docs.nuget.org/consume/package-restore#opting-out">opt out of the Automatic Package Restore</a> on Visual Studio add a <em>Nuget.config</em> in the solution root.
&#8220;` xml
&lt;?xml version=&ldquo;1.0&rdquo; encoding=&ldquo;utf-8&rdquo;?>
<configuration>
  <packageRestore>
    <!-- Opts out of both Automatic Package Restore and MSBuild-Integrated Package Restore -->
    <add key="enabled" value="False" /></p>

<pre><code>&lt;!-- Opts out of Automatic Package Restore in Visual Studio --&gt;
&lt;add key="automatic" value="False" /&gt;
</code></pre>

<p>  </packageRestore>
</configuration>
&#8220;`</p>

<p>Hope this helps you to move away from NuGet restore at build time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Checking in Package Dependencies into Source Control]]></title>
    <link href="http://rahulpnath.com/blog/checking-in-package-dependencies-into-source-control/"/>
    <updated>2016-04-26T12:21:21+10:00</updated>
    <id>http://rahulpnath.com/blog/checking-in-package-dependencies-into-source-control</id>
    <content type="html"><![CDATA[<p><em>This post looks into why we should include packages in the source control and not resolve it via configuration files at build time.</em></p>

<p>Over the past few years, <a href="https://en.wikipedia.org/wiki/Package_manager">Package Managers</a> have gained an important role in the way software gets developed. There is an <a href="https://github.com/showcases/package-managers">increasing number of package managers</a> catering to different programming languages and areas of development, making the distribution of reusable libraries and plugins easy. The convention that&rsquo;s usually followed with these package dependencies is to exclude them from source control, and use a configuration file (<a href="https://docs.npmjs.com/files/package.json">package.json</a>, <a href="https://docs.nuget.org/consume/package-restore">packages.config</a>) to retrieve all the packages at build time. Even the <a href="https://github.com/github/gitignore">GitHub’s collection</a> of <a href="https://git-scm.com/docs/gitignore">.gitignore</a> file templates ignores the packages folders of various package managers.</p>

<pre><code class="text"># NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/packages/*
...
# Dependency directories
node_modules
jspm_packages
</code></pre>

<h3>Common Arguments for not Checking in Packages</h3>

<p>Since checking in packages is not a common practice, let&rsquo;s first see some of the arguments for not doing this and how it compares to having them checked in.</p>

<h4><strong>Storage</strong></h4>

<p><em>Packages are something that can be resolved at runtime and keeping them excluded saves that extra space on the source control system.</em></p>

<p>Yes, this might have been a good reason few years back, but these days this is not a good reason as storage has become really cheap. Moreover popular source control systems charge by the <a href="https://github.com/pricing/plans">number of repositories</a> and not by the space it occupies (although it has <a href="https://help.github.com/articles/what-is-my-disk-quota/">limits</a> on it).</p>

<h4><strong>Time</strong></h4>

<p><em>The clone is faster when you do not have packages in the source control repository as opposed to having them.</em></p>

<p>But for the project to build we need the packages restored first. So the time is either spent in the clone or in the restore. But if the packages are included in the git clone then you can immediately start working on the project after a clone and do not need any internet connectivity to make the project build. This is also of advantage if you want to run a &lsquo;<a href="https://git-scm.com/docs/git-clean">git clean</a>&rsquo; - which cleans the working tree by recursively removing files that are not under version control. With packages not under the source control, you have to restore them every time you run it - This is not a problem if you have internet connectivity, but will block your work if you do it when you don&rsquo;t.</p>

<blockquote><p><em>Without checking in dependent packages, you can&rsquo;t git clone and get on a flight nor can you git clean while on a <a href="https://en.wikipedia.org/wiki/Airplane_mode">flight</a></em></p></blockquote>

<p>Moreover cloning a repository is a one-time activity, while a clean can be done any time a developer wants to. So it actually saves more time to keep the packages checked in.</p>

<h3>More Reason for Checking in Packages</h3>

<p>Now that we have seen most of the common arguments are not valid, let&rsquo;s see more reasons on why including the packages into the source control is actually better.</p>

<h4><strong>Explicit Dependencies</strong></h4>

<p>It&rsquo;s always better to be explicit about your code dependencies and not have them resolved by a package manager.</p>

<blockquote><p><em>Packages are nothing but code and can alter the behaviour of the application.</em></p></blockquote>

<p>There are possibilities of specific package versions getting <a href="http://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm">removed from the package manager</a>, which your application is still dependent on and leads to build breakage! If your package configuration is set up in such a way to resolve the latest available package of  the specific dependency, there are possibilities that the package owner pushes an update that is not backward compatible, causing the build to break! Given that these possibilities exist there is no reason to exclude package dependencies from checked in.</p>

<h4><strong>Package Source Downtime</strong></h4>

<p>Though the publicly available package sources like NuGet, npm are available almost all the time, it is likely that they too <a href="http://stackoverflow.com/questions/17806889/nuget-feed-reliability">can go down</a>. The last thing you would want is to get blocked by the downtime of these services - be it failure to build locally or on a server or even block a critical deployment. With the packages available in your source control, you have one less moving part in your whole deployment pipeline and it is better to have lesser dependencies.</p>

<h4><strong>Custom Package Sources</strong></h4>

<p>Many times I have had to update my Package sources in Visual Studio and break my head on the specific order of these entries to get the project building. This is very common when using custom packages sources like <a href="http://inedo.com/proget">ProGet</a> or <a href="https://myget.org/">MyGet</a>. Such dependencies make project setup harder and is easily avoided if all the dependent assemblies are available within the repository.  You can still have them as custom NuGet sources but have the dependencies included into the repository and update the references whenever source changes. This makes project ramp up easier and faster, with one less configuration step.</p>

<p><img class="center" alt="Nuget custom package source" src="/images/nuget_package_sources.png" /></p>

<p>Do you still see any reason for not checking in package dependencies into the source control? If not let&rsquo;s go and change that package folder exclude and have them included in the source. (I just updated <a href="http://www.rahulpnath.com/blog/clal-command-line-application-launcher/">CLAL</a> to <a href="https://github.com/rahulpnath/clal/commit/736023d9ab4bd285cb077ff54acd1bbaad142a08">include dependencies.</a>)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[git checkout TFS]]></title>
    <link href="http://rahulpnath.com/blog/git-checkout-tfs/"/>
    <updated>2016-04-15T05:23:08+10:00</updated>
    <id>http://rahulpnath.com/blog/git-checkout-tfs</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been a year since using <a href="https://git-scm.com/">Git</a> as my mainstream version control system and I am loving it! Before Git, I had used Team Foundation Version Control (TFVC) for a very long time and was so used to it that I found Git a bit complex and overwhelming in the beginning. Team Foundation Server (TFS) is the whole product suite from Microsoft that provides source code management. Until TFS 2013, it supported only TFVC which is when it introduced <a href="https://blogs.msdn.microsoft.com/mvpawardprogram/2013/11/13/git-for-tfs-2013/">Git in TFS</a>. Even today people use TFS and TFVC synonymously (like in the title of this post) though they are not the same.</p>

<h3>Fundamental shift in thinking</h3>

<p>By design, Git is a Distributed VCS, whereas TFS is centralized one. It takes quite a while to get your head around this and what it actually means. By definition</p>

<blockquote><p><em><strong>TFVC</strong>: Uses a single, centralized server repository to track and version files. Local changes are always checked in to the central server where other developers can get the latest changes.</em></p>

<p><em><strong>Git</strong>: Git is a distributed version control system. Each developer has a copy of the source repository on their dev machine. Developers can commit each set of changes on their dev machine and perform version control operations such as history and compare without a network connection.</em></p></blockquote>

<p>You. may not see the real Distributed benefits if you are working off a central repository (hosted on a server like GitHub or Bitbucket) and using <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/centralized-workflow">TFS way of development</a> :</p>

<p><em>Get latest code => Make your changes => Merge latest +> Check in (</em>Commit and push<em>)</em></p>

<p>The real power of Git is better understood when you start working disconnected, use branches to keep unrelated development activities separate and merge those into the main trunk (<em>master</em>) once comfortable. You get a local copy of the project and lets you make changes independent of all the other changes in the project.</p>

<blockquote><p><em>Git feels so lightweight and never gets in the way of doing things.</em></p></blockquote>

<h3>Make command line your friend</h3>

<p>If you are a UI savvy person then Git might a good starting point to start using the command line. At first, it definitely feels hard especially if you were TFS/Visual Studio users and might be tempted to use the GUI tools available (<a href="https://desktop.github.com/">GitHub Desktop</a> or <a href="https://www.sourcetreeapp.com/">SourceTree</a>)</p>

<blockquote><p><em>Repetitive tasks become more evident when you use a command line and easily automatable.</em></p></blockquote>

<p>I use Cmder (<a href="http://www.rahulpnath.com/blog/tools-that-I-use/">one of my favourite tools</a>) with Git and have <a href="https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/">set up SSH</a> to Bitbucket and Github (expected soon on TFS), secured by a paraphrase, so that I do not have to key in the credentials every time I interact with the repositories. I <a href="https://github.com/cmderdev/cmder/issues/193#issuecomment-63040989">start the ssh-agent the very first time I open Cmder</a>, which prompts for my paraphrase and continues to run in the background. Alternatively, you can also use <a href="https://github.com/Microsoft/Git-Credential-Manager-for-Windows">Credential Manager</a> to store credentials, when working with HTTP enabled Git repository. For the common commands, I have set up aliases like below, to save a bit on the keystrokes.</p>

<pre><code class="text">gl=git log --oneline --all --graph --decorate  $*
gs=git status
ga=git add -A 
gp=git pull
gpp=git push 
gc=git commit -m "$*"  
gcc=git commit
</code></pre>

<h3>Different workflows</h3>

<p>Git can be used in many ways and which makes it hard to get started. There are a few popular <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/">workflows</a> that one can use. Currently, I am using -  <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow">Feature Branch workflow</a> -
which means that all work happen on independent feature branches and once completed gets merged into the main trunk (master branch). Code Reviews happens on the way it gets pulled into the main branch, which ensures code quality and familiarity.</p>

<p><a href="https://www.atlassian.com/git/images/tutorials/collaborating/comparing-workflows/feature-branch-workflow/01.svg"><img class="center" alt="Git Feature Branch Workflow" src="/images/git_featurebranch_workflow.png" /></a></p>

<h3>Not Just for Code</h3>

<p>Git is a version control system and does not limit itself to storing code. You can use it for <a href="http://readwrite.com/2013/11/08/seven-ways-to-use-github-that-arent-coding/">version controlling any of your work</a>. For example, this blog is <a href="https://github.com/rahulpnath/rahulpnath.com">hosted on Github</a> and all the <a href="https://github.com/rahulpnath/rahulpnath.com/commits/master">changes are version controlled</a>, which gives me the flexibility to work and commit locally. Since the blog is <a href="http://www.rahulpnath.com/blog/static-generator-is-all-a-blog-needs-moving-to-octopress/">static generated</a> I can also preview all the changes locally. I use git whenever I work on any documents or <a href="https://github.com/rahulpnath/Speaking">presentations</a> so that I can avoid manual copy of files and renaming with suffixes like &lsquo;<em>Draft</em>, &rsquo;<em>Draft1</em>,<em>Final</em>,&ldquo;<em>FinalRevision</em>&lsquo; etc. (if that sounds any similar)</p>

<h3>Managing Commits</h3>

<p>When coming to commits, which are nothing but checkpoints of meaningful work done, people might have a different  definition for &lsquo;<em>meaningful</em>&rsquo; - for some it might be really granular, for others a bit coarse and for yet another it means all the work is done. I tend to commit quite often - even a rename of a variable leads to a commit so that I do not have to backtrack if at all something goes wrong immediately after that.</p>

<p>if you really like the idea of committing often (locally), but want the pushes to remotes more coarse, you can &lsquo;<strong><a href="http://stackoverflow.com/questions/5189560/squash-my-last-x-commits-together-using-git">squash your commits</a></strong>&rsquo;, before pushing it to remote branch. This allows you to commit often locally and still push  meaningful commit in the main source history. Make sure that the <a href="http://chris.beams.io/posts/git-commit/">commit messages and clear and communicates the intent</a> and helps <a href="http://megakemp.com/2014/08/14/the-importance-of-a-good-looking-history/">keep a good looking history</a>.</p>

<p>Git is one of the best things that happened to developers and hopes it stays long!</p>

<p><strong>References</strong></p>

<ul>
<li><a href="http://gitref.org/index.html">Git Reference</a></li>
<li><a href="https://git-scm.com/book/en/v2">Pro Git</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
